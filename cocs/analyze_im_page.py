#!/usr/bin/env python3
"""
åˆ†æå’¸é±¼IMé¡µé¢HTMLï¼Œæ‰¾å‡ºç™»å½•åˆ¤æ–­å’Œæ–°æ¶ˆæ¯è¯†åˆ«çš„patterns
"""
import re
import json
from bs4 import BeautifulSoup
from collections import defaultdict


def analyze_im_page():
    """åˆ†æIMé¡µé¢HTMLç»“æ„"""

    print("ğŸ” å¼€å§‹åˆ†æå’¸é±¼IMé¡µé¢HTMLç»“æ„...")

    # è¯»å–HTMLæ–‡ä»¶
    try:
        with open('/Users/jimmypan/git_repo/XianyuAutoAgent/cocs/debug_pages/debug_pages.html', 'r', encoding='utf-8') as f:
            html_content = f.read()
    except Exception as e:
        print(f"âŒ æ— æ³•è¯»å–HTMLæ–‡ä»¶: {e}")
        return

    print(f"ğŸ“Š HTMLæ–‡ä»¶å¤§å°: {len(html_content)} å­—ç¬¦")

    # ä½¿ç”¨BeautifulSoupè§£æ
    soup = BeautifulSoup(html_content, 'html.parser')

    # 1. å¯»æ‰¾è”ç³»äººç›¸å…³çš„pattern
    print("\nğŸ¯ 1. åˆ†æè”ç³»äººåˆ—è¡¨patterns...")
    analyze_contact_patterns(soup, html_content)

    # 2. å¯»æ‰¾æ¶ˆæ¯ç›¸å…³çš„pattern
    print("\nğŸ¯ 2. åˆ†ææ¶ˆæ¯patterns...")
    analyze_message_patterns(soup, html_content)

    # 3. å¯»æ‰¾æ–°æ¶ˆæ¯æ ‡è®°patterns
    print("\nğŸ¯ 3. åˆ†ææ–°æ¶ˆæ¯æ ‡è®°patterns...")
    analyze_notification_patterns(soup, html_content)

    # 4. å¯»æ‰¾ç™»å½•çŠ¶æ€åˆ¤æ–­patterns
    print("\nğŸ¯ 4. åˆ†æç™»å½•çŠ¶æ€patterns...")
    analyze_login_patterns(soup, html_content)


def analyze_contact_patterns(soup, html_content):
    """åˆ†æè”ç³»äººåˆ—è¡¨patterns"""

    # æœç´¢åŒ…å«è”ç³»äººåå­—çš„å…ƒç´ 
    target_names = ['å…‰å½±ç§Ÿç•Œ', 'ç«å±±è°¦è™šçš„å±±èŒ¶', 'è¦å¥½è´§1818', 'æ¶ˆæ¯é€šçŸ¥']

    contact_info = {}

    for name in target_names:
        print(f"\nğŸ” æœç´¢è”ç³»äºº: {name}")

        # åœ¨HTMLä¸­æœç´¢è¿™ä¸ªåå­—
        pattern = re.escape(name)
        matches = list(re.finditer(pattern, html_content))

        if matches:
            print(f"  æ‰¾åˆ° {len(matches)} ä¸ªåŒ¹é…")

            for i, match in enumerate(matches[:2]):  # åªåˆ†æå‰2ä¸ªåŒ¹é…
                start_pos = max(0, match.start() - 500)
                end_pos = min(len(html_content), match.end() + 500)
                context = html_content[start_pos:end_pos]

                # åˆ†æå‘¨å›´çš„HTMLç»“æ„
                analyze_surrounding_structure(context, name, match.start() - start_pos)
        else:
            print(f"  âŒ æœªæ‰¾åˆ°")

    # å¯»æ‰¾å¯èƒ½çš„è”ç³»äººåˆ—è¡¨å®¹å™¨
    print(f"\nğŸ” å¯»æ‰¾è”ç³»äººåˆ—è¡¨å®¹å™¨...")

    # å¸¸è§çš„è”ç³»äººåˆ—è¡¨é€‰æ‹©å™¨
    potential_containers = [
        'div[class*="contact"]',
        'div[class*="conversation"]',
        'div[class*="chat"]',
        'ul[class*="list"]',
        'div[class*="sidebar"]',
        '[class*="user"]'
    ]

    for selector in potential_containers:
        elements = soup.select(selector)
        if elements:
            print(f"  ğŸ“‹ {selector}: æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")
            for i, elem in enumerate(elements[:3]):
                print(f"    [{i+1}] class='{elem.get('class', [])}' tag='{elem.name}'")


def analyze_surrounding_structure(context, name, name_pos):
    """åˆ†æåå­—å‘¨å›´çš„HTMLç»“æ„"""

    print(f"\n    ğŸ“ åˆ†æ '{name}' å‘¨å›´çš„ç»“æ„:")

    # è§£æè¿™ä¸€æ®µHTML
    try:
        soup = BeautifulSoup(context, 'html.parser')

        # æ‰¾åˆ°åŒ…å«åå­—çš„å…ƒç´ 
        elements_with_name = soup.find_all(string=re.compile(re.escape(name)))

        for elem_text in elements_with_name[:1]:  # åªåˆ†æç¬¬ä¸€ä¸ª
            parent = elem_text.parent

            # å‘ä¸ŠæŸ¥æ‰¾åŒ…å«å…ƒç´ çš„ç»“æ„
            current = parent
            level = 0
            while current and level < 5:
                tag_info = f"{current.name}"
                if current.get('class'):
                    tag_info += f".{'.'.join(current.get('class', []))}"
                if current.get('id'):
                    tag_info += f"#{current.get('id')}"

                print(f"      {'  ' * level}â”œâ”€ {tag_info}")

                # æ£€æŸ¥æ˜¯å¦æœ‰å…„å¼Ÿå…ƒç´ ï¼ˆå¯èƒ½æ˜¯å…¶ä»–è”ç³»äººï¼‰
                if level == 2:  # åœ¨é€‚å½“å±‚çº§æ£€æŸ¥å…„å¼Ÿå…ƒç´ 
                    siblings = current.find_next_siblings()
                    if siblings:
                        print(f"      {'  ' * level}   å…„å¼Ÿå…ƒç´ : {len(siblings)} ä¸ª")

                current = current.parent
                level += 1

    except Exception as e:
        print(f"      âš ï¸ è§£æé”™è¯¯: {e}")


def analyze_message_patterns(soup, html_content):
    """åˆ†ææ¶ˆæ¯patterns"""

    # å¯»æ‰¾å¯èƒ½çš„æ¶ˆæ¯å®¹å™¨
    message_selectors = [
        'div[class*="message"]',
        'div[class*="conversation"]',
        'li[class*="item"]',
        '[class*="bubble"]',
        '[class*="content"]'
    ]

    for selector in message_selectors:
        elements = soup.select(selector)
        if elements:
            print(f"  ğŸ“ {selector}: æ‰¾åˆ° {len(elements)} ä¸ªå…ƒç´ ")

            # åˆ†æå‰å‡ ä¸ªå…ƒç´ çš„ç»“æ„
            for i, elem in enumerate(elements[:3]):
                classes = elem.get('class', [])
                text_content = elem.get_text(strip=True)[:50]

                print(f"    [{i+1}] classes: {classes}")
                if text_content:
                    print(f"         text: '{text_content}...'")


def analyze_notification_patterns(soup, html_content):
    """åˆ†ææ–°æ¶ˆæ¯é€šçŸ¥patterns"""

    # å¯»æ‰¾å¾½ç« ã€çº¢ç‚¹ã€æ•°å­—ç­‰æ–°æ¶ˆæ¯æ ‡è®°
    notification_patterns = [
        # å¾½ç« ç›¸å…³
        r'badge[^"]*',
        r'count[^"]*',
        r'unread[^"]*',
        r'notification[^"]*',
        # æ•°å­—ç›¸å…³
        r'(\d+)',
        # çº¢ç‚¹ç›¸å…³
        r'dot[^"]*',
        r'indicator[^"]*'
    ]

    print("  ğŸ”” æœç´¢æ–°æ¶ˆæ¯æ ‡è®°patterns:")

    # æœç´¢classåç§°patterns
    class_pattern = r'class="([^"]*)"'
    all_classes = re.findall(class_pattern, html_content)

    # ç»Ÿè®¡ç›¸å…³çš„class
    relevant_classes = defaultdict(int)

    for class_str in all_classes:
        classes = class_str.split()
        for cls in classes:
            # æ£€æŸ¥æ˜¯å¦åŒ…å«é€šçŸ¥ç›¸å…³å…³é”®è¯
            cls_lower = cls.lower()
            if any(keyword in cls_lower for keyword in ['badge', 'count', 'unread', 'dot', 'notification', 'indicator', 'num']):
                relevant_classes[cls] += 1

    # è¾“å‡ºç›¸å…³ç±»å
    if relevant_classes:
        print("    ğŸ“Š å‘ç°çš„é€šçŸ¥ç›¸å…³class:")
        for cls, count in sorted(relevant_classes.items(), key=lambda x: x[1], reverse=True)[:10]:
            print(f"      {cls}: {count}æ¬¡")

    # æœç´¢ç‰¹å®šçš„å¾½ç« å…ƒç´ 
    badge_elements = soup.select('[class*="badge"], [class*="count"], [class*="unread"], [class*="dot"]')
    print(f"    ğŸ¯ å¾½ç« ç±»å…ƒç´ : {len(badge_elements)} ä¸ª")

    for i, elem in enumerate(badge_elements[:5]):
        classes = elem.get('class', [])
        text = elem.get_text(strip=True)
        print(f"      [{i+1}] {elem.name}.{'.'.join(classes)}: '{text}'")


def analyze_login_patterns(soup, html_content):
    """åˆ†æç™»å½•çŠ¶æ€åˆ¤æ–­patterns"""

    print("  ğŸ” åˆ†æç™»å½•çŠ¶æ€æŒ‡æ ‡:")

    # æ£€æŸ¥é¡µé¢æ ‡é¢˜å’ŒåŸºæœ¬ä¿¡æ¯
    title = soup.find('title')
    if title:
        print(f"    ğŸ“„ é¡µé¢æ ‡é¢˜: '{title.get_text(strip=True)}'")

    # æ£€æŸ¥HTMLæ ¹å…ƒç´ çš„class
    html_elem = soup.find('html')
    if html_elem:
        html_classes = html_elem.get('class', [])
        print(f"    ğŸ·ï¸ HTML class: {html_classes}")

    # å¯»æ‰¾ç”¨æˆ·ä¿¡æ¯ç›¸å…³å…ƒç´ 
    user_indicators = [
        '[class*="user"]',
        '[class*="avatar"]',
        '[class*="profile"]',
        '[class*="account"]'
    ]

    for selector in user_indicators:
        elements = soup.select(selector)
        if elements:
            print(f"    ğŸ‘¤ {selector}: {len(elements)} ä¸ªå…ƒç´ ")

    # æ£€æŸ¥æ˜¯å¦æœ‰ç™»å½•ç›¸å…³çš„å…ƒç´ 
    login_indicators = soup.select('[class*="login"], [class*="signin"], [class*="auth"]')
    print(f"    ğŸ”‘ ç™»å½•ç›¸å…³å…ƒç´ : {len(login_indicators)} ä¸ª")

    # æ£€æŸ¥é¡µé¢URLæ¨¡å¼ï¼ˆä»metaæˆ–scriptä¸­æ¨æ–­ï¼‰
    url_patterns = re.findall(r'https?://[^"\'>\s]+', html_content)
    im_urls = [url for url in url_patterns if 'im' in url or 'chat' in url or 'message' in url]
    print(f"    ğŸŒ IMç›¸å…³URL: {len(im_urls)} ä¸ª")


def generate_selectors():
    """åŸºäºåˆ†æç»“æœç”Ÿæˆé€‰æ‹©å™¨å»ºè®®"""

    print("\nğŸ’¡ åŸºäºåˆ†æç”Ÿæˆçš„é€‰æ‹©å™¨å»ºè®®:")

    suggestions = {
        "ç™»å½•æ£€æµ‹": [
            "html.page-im",  # åŸºäºHTML class
            "[class*='conversation']",
            "[class*='contact']",
            "[class*='user']"
        ],
        "è”ç³»äººåˆ—è¡¨": [
            "div[class*='contact']",
            "div[class*='conversation']",
            "ul[class*='list']",
            "li[class*='item']"
        ],
        "æ–°æ¶ˆæ¯æ ‡è®°": [
            "[class*='badge']",
            "[class*='count']",
            "[class*='unread']",
            "[class*='dot']",
            "[class*='notification']"
        ],
        "æ¶ˆæ¯å†…å®¹": [
            "div[class*='message']",
            "div[class*='content']",
            "[class*='bubble']"
        ]
    }

    for category, selectors in suggestions.items():
        print(f"\n  ğŸ“‹ {category}:")
        for selector in selectors:
            print(f"    - {selector}")


if __name__ == "__main__":
    analyze_im_page()
    generate_selectors()